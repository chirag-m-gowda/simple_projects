{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chirag-m-gowda/simple_projects/blob/main/Speech_Recognition_with_RNN_Neural_Networks_colab_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9WsXpBv-Zmn"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import urllib.request\n",
        "import pathlib\n",
        "import shutil\n",
        "import os\n",
        "import librosa\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import sys, os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import copy \n",
        "import torch\n",
        "# display\n",
        "import IPython.display as ipd\n",
        "from IPython.core.display import HTML\n",
        "from IPython.display import display, clear_output\n",
        "# audio library\n",
        "import librosa.display \n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGz-R5Kb-Zmr"
      },
      "source": [
        "\n",
        "# 1. Introduction\n",
        "\n",
        "The goal of this project is to implement an audio classification system, which: \n",
        "1. first reads in an audio clip (containing at most one word),\n",
        "2. and then recognizes the class(label) of this audio.\n",
        "\n",
        "\n",
        "### Classes  \n",
        "11 classes are chosen, namely:   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOIUz9Un-Zmt"
      },
      "outputs": [],
      "source": [
        "classes=[\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9b-Vxu8-Zmt"
      },
      "source": [
        "where \"yes\" means the audio contains the word \"yes\", etc. \n",
        "### Method  \n",
        "\n",
        "Features: MFCCs (Mel-frequency cepstral coefficients) are computed from the raw audio. You can think of it as the result of fouriour transformation.\n",
        "\n",
        "Classifier: LSTM (Long Short-Term Memory) is adopted for classificatioin, which is a type of Recurrent Neural Network.\n",
        "\n",
        "The model was pretrained on the [Speech Commands Dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) with intensive data augmentation, including \"shift\", \"amplify\", \"superpose noise\", etc.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCyHTeEd-Zmu"
      },
      "source": [
        "## Step 2 Creation of some utility programs\n",
        "\n",
        "We define some functions that allow us download the datasets that we need to use to create our ML model and train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkzPgeJb-Zmu"
      },
      "outputs": [],
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(\n",
        "            url, filename=output_path, reporthook=t.update_to)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaWzW15k-Zmv"
      },
      "source": [
        "### 3.1.1. Speech Commands Dataset \n",
        "[Speech Commands Dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) contains **105k audio clips** (3.3GB large) collected from lots of people.  \n",
        "It has **35 classes** of words, listed as follows:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YVNzJRb-Zmw"
      },
      "outputs": [],
      "source": [
        "# We define some parameters\n",
        "# current working directory\n",
        "DIR = os.path.abspath(os.getcwd())\n",
        "DATASET_DIRECTORY_PATH = DIR+'/data/speech_commands'\n",
        "#DOWNLOAD_URL = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "DOWNLOAD_URL = \"http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE5k7Di0-Zmx"
      },
      "source": [
        "### Downloading the data and  Unzip the tar file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4L7z4-W-Zmx"
      },
      "outputs": [],
      "source": [
        "# Check if dataset directory already exist, otherwise download, extract and remove the archive\n",
        "if not os.path.isdir(DATASET_DIRECTORY_PATH):\n",
        "    if not os.path.isdir(DIR+'/data'):\n",
        "        os.mkdir(DIR+'/data')\n",
        "    print('Downloading from ' + DOWNLOAD_URL)\n",
        "    download_file(DOWNLOAD_URL, DIR+'/data/speech_commands.tar.gz')\n",
        "    print(\"Extracting archive...\")\n",
        "    shutil.unpack_archive(\n",
        "        DIR+'/data/speech_commands.tar.gz', DATASET_DIRECTORY_PATH)\n",
        "    os.remove(DIR+'/data/speech_commands.tar.gz')\n",
        "    print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HunZcbQ3-Zmy"
      },
      "outputs": [],
      "source": [
        "# Delete the extra files of extracted file\n",
        "# Cleaning data\n",
        "if os.name == 'nt':\n",
        "    print(\"We are on Windows\")\n",
        "    paths=DIR+'\\data\\speech_commands'\n",
        "    os.chdir(paths)\n",
        "    files=['testing_list.txt','validation_list.txt','LICENSE','README.md']\n",
        "    for f in files:\n",
        "        try:\n",
        "            os.remove(f)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "    #!dir\n",
        "    os.chdir(DIR)\n",
        "else:\n",
        "    print(\"We are on Unix\")\n",
        "    extras=DIR+'/data/speech_commands/*.*'\n",
        "    command='rm -rf '+ extras\n",
        "    os.system(command)\n",
        "    extras=DIR+'/data/speech_commands/LICENSE'\n",
        "    command='rm -rf '+ extras\n",
        "    os.system(command)\n",
        "    #!ls ./data/speech_commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iZ7qVvi-Zmz"
      },
      "outputs": [],
      "source": [
        "train_audio_path =DATASET_DIRECTORY_PATH+\"/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EINXxjWm-Zmz"
      },
      "outputs": [],
      "source": [
        "# Number of recording of each voices\n",
        "labels = os.listdir(train_audio_path)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M14VVM7-Zm0"
      },
      "outputs": [],
      "source": [
        "to_remove = [x for x in labels if x not in classes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udPYNp6H-Zm0"
      },
      "outputs": [],
      "source": [
        "len(to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5pi8IJb-Zm0"
      },
      "outputs": [],
      "source": [
        "print(to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPMh0psV-Zm0"
      },
      "outputs": [],
      "source": [
        "for directory in to_remove:\n",
        "    noise_dir_new=DIR+'/data/'+directory\n",
        "    noise_dir_old=DIR+'/data/speech_commands/'+directory\n",
        "    shutil.move(noise_dir_old, noise_dir_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGhSQ2hE-Zm1"
      },
      "outputs": [],
      "source": [
        "# Number of recording of each voices\n",
        "labels = os.listdir(train_audio_path)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWE18CVs-Zm1"
      },
      "outputs": [],
      "source": [
        "WORK_DIR =['data_train','checkpoints']\n",
        "for DIRECTORY in WORK_DIR:\n",
        "    WORK_DIRECTORY_PATH = DIR +'/'+ DIRECTORY \n",
        "    if not os.path.isdir(WORK_DIRECTORY_PATH):\n",
        "        if not os.path.isdir(DIR+'/'+DIRECTORY ):\n",
        "            os.mkdir(DIR+'/'+DIRECTORY )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-onC5R8-Zm1"
      },
      "outputs": [],
      "source": [
        "CONFIG_DIRECTORY_PATH = DIR+'/config'\n",
        "# Check if config directory already exist, otherwise will create\n",
        "import os\n",
        "import shutil\n",
        "if not os.path.isdir(CONFIG_DIRECTORY_PATH):\n",
        "    if not os.path.isdir(DIR+'/config'):\n",
        "        os.mkdir(DIR+'/config')\n",
        "    print('Creating config')\n",
        "    # list of names\n",
        "    names = labels\n",
        "    # open file in write mode\n",
        "    with open(r'config/classes.names', 'w') as fp:\n",
        "        for item in names:\n",
        "            # write each item on a new line\n",
        "            fp.write(\"%s\\n\" % item)\n",
        "    print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO1hnUhY-Zm2"
      },
      "source": [
        "### Downloading the utils and installing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDZYfzgM-Zm2"
      },
      "outputs": [],
      "source": [
        "LIB_DIRECTORY_PATH = DIR+'/utils'\n",
        "# Check if utils directory already exist, otherwise download, and install\n",
        "import os\n",
        "import shutil\n",
        "if not os.path.isdir(LIB_DIRECTORY_PATH):\n",
        "    if not os.path.isdir(DIR+'/utils'):\n",
        "        os.mkdir(DIR+'/utils')\n",
        "    print('Downloading utils')\n",
        "    user = \"ruslanmv\"\n",
        "    repo = \"Speech-Recognition-with-RNN-Neural-Networks\"\n",
        "    src_dir = \"utils\"\n",
        "    pyfile = \"lib.py\"\n",
        "    url = f\"https://raw.githubusercontent.com/{user}/{repo}/master/{src_dir}/{pyfile}\"\n",
        "    !wget --no-cache --backups=1 {url}\n",
        "    print(\"Installing library...\")\n",
        "    shutil.move(DIR+'/lib.py', DIR +'/utils/lib.py')\n",
        "    print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r39wGijXIATH"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import  gtts\n",
        "except ImportError as e:\n",
        "    !pip install gtts\n",
        "    pass  # module doesn't exist, deal with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNvGdTZf-Zm2"
      },
      "outputs": [],
      "source": [
        "# Import all libraries\n",
        "import utils.lib as lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt5y7yW9-Zm2"
      },
      "source": [
        "# 3. Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cgpTWl-Zm3"
      },
      "source": [
        "## 3.1. Training data   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R9U3cr5-Zm3"
      },
      "source": [
        "These large amount of data are important for extracting the core features of a word.  \n",
        "If I didn't do the pretraining on this dataset, the model will have poor performance when generalizing to other people's voices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO5R1eRM-Zm3"
      },
      "source": [
        "### 3.1.2. My dataset \n",
        "We choose 10 target classes listed as follows:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5lS7N7C-Zm3"
      },
      "outputs": [],
      "source": [
        "# Number of recording of each voices\n",
        "labels = os.listdir(train_audio_path)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3JOzYxF-Zm3"
      },
      "outputs": [],
      "source": [
        "len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x02sxu8U-Zm3"
      },
      "outputs": [],
      "source": [
        "labels=os.listdir(train_audio_path)\n",
        "#find count of each label and plot bar graph\n",
        "no_of_recordings=[]\n",
        "for label in labels:\n",
        "    waves = [f for f in os.listdir(train_audio_path + label) if f.endswith('.wav')]\n",
        "    no_of_recordings.append(len(waves))\n",
        "    \n",
        "#plot\n",
        "plt.figure(figsize=(30,5))\n",
        "index = np.arange(len(labels))\n",
        "plt.bar(index, no_of_recordings)\n",
        "plt.xlabel('Commands', fontsize=12)\n",
        "plt.ylabel('No of recordings', fontsize=12)\n",
        "plt.xticks(index, labels, fontsize=15, rotation=60)\n",
        "plt.title('No. of recordings for each command')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zj6Enmn-Zm4"
      },
      "outputs": [],
      "source": [
        "#Load the audio file\n",
        "if os.name == 'nt':\n",
        "    print(\"We are on Windows\")\n",
        "    test_file=train_audio_path+'left/00b01445_nohash_0.wav'\n",
        "    audio = lib.AudioClass(filename=test_file)\n",
        "else:\n",
        "    print(\"We are on Unix\")\n",
        "    test_file=train_audio_path+'left/00b01445_nohash_0.wav'\n",
        "    audio = lib.AudioClass(filename=test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKtO5NN4-Zm4"
      },
      "outputs": [],
      "source": [
        "ipd.Audio(audio.data, rate=audio.sample_rate) # play audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb5P30f4-Zm4"
      },
      "source": [
        "## 3.2. Data augmentation\n",
        "\n",
        "Raw data goes through a serials of augmentation before training, including:  \n",
        "> Shift  \n",
        "> Pad zeros  \n",
        "> Amplify  \n",
        "> Change play speed  \n",
        "> Superpose noise  \n",
        "\n",
        "This step is essential. Raw audios in Speech Commands Dataset are all about 1 second long, and have little background noise. It can easily causes certain overfitting.  \n",
        "\n",
        "We should try to diversify the data by data augmentation techniques, in order to get prepared for all kinds of real-world scenarios, including noise, variation of the audio length and loudness. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AsBaQpq-Zm5"
      },
      "source": [
        "**Example of audio augmentation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFEYrmDM-Zm5"
      },
      "outputs": [],
      "source": [
        "# Initialize the augmenter.\n",
        "#     Specify a parameter's range for generating a random augmentation.\n",
        "\n",
        "Aug = lib.Augmenter\n",
        "aug = Aug([        \n",
        "    \n",
        "    Aug.Shift(rate=(0, 0.2), keep_size=False), # shift data for 0~0.2 percent of the total length\n",
        "    \n",
        "    Aug.PadZeros(time=(0, 0.3)),  # pad zeros at one side for 0~0.3 seconds \n",
        "    \n",
        "    Aug.Amplify(rate=(0.2, 1.5)), # amplify loudness by 0.2~1.5 \n",
        "    \n",
        "    Aug.PlaySpeed(rate=(0.7, 1.3), keep_size=False), # change play speed\n",
        "    \n",
        "    Aug.Noise( # Superpose noise.\n",
        "        # (Noise files are pre-load and normalized)\n",
        "        noise_folder=\"data/_background_noise_/\", prob_noise=1.0, intensity=(0, 0.7)),\n",
        "\n",
        "], prob_to_aug=1.0, # probability to do this augmentation\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppntVQyE-Zm5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Read audio, do two different augmentations, and plot results\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "plt.subplot(131)\n",
        "audio_1 = lib.AudioClass(filename=test_file)\n",
        "audio_1.plot_audio(ax=plt.gca())\n",
        "plt.title(\"Raw audio\")\n",
        "\n",
        "plt.subplot(132)\n",
        "audio_2 = copy.deepcopy(audio_1)\n",
        "aug(audio_2) # augment audio\n",
        "audio_2.plot_audio(ax=plt.gca())\n",
        "plt.title(\"Augmentation 1\")\n",
        "\n",
        "plt.subplot(133)\n",
        "audio_3 = copy.deepcopy(audio_1)\n",
        "aug(audio_3) # augment audio\n",
        "audio_3.plot_audio(ax=plt.gca())\n",
        "plt.title(\"Augmentation 2\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxH6_DVy-Zm5"
      },
      "outputs": [],
      "source": [
        "ipd.Audio(audio_1.data, rate=audio_1.sample_rate) # play audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KhvCH2D-Zm5"
      },
      "outputs": [],
      "source": [
        "ipd.Audio(audio_2.data, rate=audio_2.sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx9MQuxi-Zm6"
      },
      "outputs": [],
      "source": [
        "ipd.Audio(audio_3.data, rate=audio_3.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnv2EYT5-Zm6"
      },
      "source": [
        "**Noise files for audio augmentation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF0wx04_-Zm6"
      },
      "outputs": [],
      "source": [
        "audio_noise = lib.AudioClass(filename=\"data/_background_noise_/doing_the_dishes.wav\")\n",
        "ipd.Audio(audio_noise.data, rate=audio_noise.sample_rate) # play audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-TH05FX-Zm6"
      },
      "source": [
        "## 3.3. Features\n",
        "The [MFCCs](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) feature is computed and fed into the classifer.\n",
        "\n",
        "An intuitive understanding of MFCCs is: Use a sliding window on the raw data, and compute the fourior transform of each window to obtain the \"loudness\" on each frequency band.\n",
        "\n",
        "The code and parameters for computing MFCCs is:\n",
        "\n",
        "``` python\n",
        "def compute_mfcc(data, sample_rate, n_mfcc=12):\n",
        "    return librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYbWMDwb-Zm6",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# A visualization of the MFCCs features is shown below:\n",
        "audio.compute_mfcc()\n",
        "audio.plot_audio_and_mfcc()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx5Yemxt-Zm6"
      },
      "source": [
        "* Histogram of MFCCs is a bad feature  \n",
        "I also tested the feature of \"the histogram of MFCCs\", but the performance is bad, since the information of time length is lost.  \n",
        "A histogram feature is shown below. It's computed piece-wisely of 3 pieces on a MFCCs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAk9yeYs-Zm7"
      },
      "outputs": [],
      "source": [
        "audio.compute_mfcc_histogram(bins=10, binrange=(-50, 200), col_divides=3)\n",
        "audio.plot_mfcc_histogram() # After experiment, I found that this feature is bad. Don't use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqiKrgmv-Zm7"
      },
      "source": [
        "# 3.4. Classifier\n",
        "The input audio has a varying length, thus the feature's length also varies.  \n",
        "To deal with such cases, I adopted the LSTM (Long short-term memory) for classification.  \n",
        "The main paramters are set as: 3 layers and 64 states in each layer.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxThysbd-Zm7"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "ROOT=DATASET_DIRECTORY_PATH\n",
        "sys.path.append(ROOT)\n",
        "import numpy as np \n",
        "import torch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUTgaCoy-Zm7"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import time\n",
        "import types\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoC9v19f-Zm7"
      },
      "outputs": [],
      "source": [
        "def set_default_args():\n",
        "    \n",
        "    args = types.SimpleNamespace()\n",
        "\n",
        "    # model params\n",
        "    args.input_size = 12  # == n_mfcc\n",
        "    args.batch_size = 1\n",
        "    args.hidden_size = 64\n",
        "    args.num_layers = 3\n",
        "\n",
        "    # training params\n",
        "    args.num_epochs = 100\n",
        "    args.learning_rate = 0.0001\n",
        "    args.learning_rate_decay_interval = 5 # decay for every 5 epochs\n",
        "    args.learning_rate_decay_rate = 0.5 # lr = lr * rate\n",
        "    args.weight_decay = 0.00\n",
        "    args.gradient_accumulations = 16 # number of gradient accums before step\n",
        "    \n",
        "    # training params2\n",
        "    args.load_weights_from = None\n",
        "    args.finetune_model = False # If true, fix all parameters except the fc layer\n",
        "    args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # data\n",
        "    args.data_folder = \"data/data_train/\"\n",
        "    args.train_eval_test_ratio=[0.9, 0.1, 0.0]\n",
        "    args.do_data_augment = False\n",
        "\n",
        "    # labels\n",
        "    #args.classes_txt = \"config/classes.names\" \n",
        "    args.classes_txt =labels\n",
        "    args.num_classes = None # should be added with a value somewhere, like this:\n",
        "    #                = len(lib.read_list(args.classes_txt))\n",
        "\n",
        "    # log setting\n",
        "    args.plot_accu = True # if true, plot accuracy for every epoch\n",
        "    args.show_plotted_accu = False # if false, not calling plt.show(), so drawing figure in background\n",
        "    args.save_model_to = 'checkpoints/' # Save model and log file\n",
        "        #e.g: model_001.ckpt, log.txt, log.jpg\n",
        "    \n",
        "    return args "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2r8sebW-Zm8"
      },
      "outputs": [],
      "source": [
        "def load_weights(model, weights, PRINT=False):\n",
        "    \n",
        "    for i, (name, param) in enumerate(weights.items()):\n",
        "        model_state = model.state_dict()\n",
        "        \n",
        "        if name not in model_state:\n",
        "            print(\"-\"*80)\n",
        "            print(\"weights name:\", name) \n",
        "            print(\"RNN states names:\", model_state.keys()) \n",
        "            assert 0, \"Wrong weights file\"\n",
        "            \n",
        "        model_shape = model_state[name].shape\n",
        "        if model_shape != param.shape:\n",
        "            print(f\"\\nWarning: Size of {name} layer is different between model and weights. Not copy parameters.\")\n",
        "            print(f\"\\tModel shape = {model_shape}, weights' shape = {param.shape}.\")\n",
        "        else:\n",
        "            model_state[name].copy_(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVj8anra-Zm8"
      },
      "outputs": [],
      "source": [
        "def create_RNN_model(args, load_weights_from=None):\n",
        "    ''' A wrapper for creating a 'class RNN' instance '''\n",
        "    # Update some dependent args\n",
        "    #args.num_classes = len(lib.read_list(args.classes_txt)) # read from \"config/classes.names\"\n",
        "    args.num_classes = len(labels) # read from \"config/classes.names\"\n",
        "    args.save_log_to = args.save_model_to + \"log.txt\"\n",
        "    args.save_fig_to = args.save_model_to + \"fig.jpg\"\n",
        "    \n",
        "    # Create model\n",
        "    device = args.device\n",
        "    model = RNN(args.input_size, args.hidden_size, args.num_layers, args.num_classes, device).to(device)\n",
        "    \n",
        "    # Load weights\n",
        "    if load_weights_from:\n",
        "        print(f\"Load weights from: {load_weights_from}\")\n",
        "        weights = torch.load(load_weights_from)\n",
        "        load_weights(model, weights)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJOhJAKw-Zm8"
      },
      "outputs": [],
      "source": [
        "# Recurrent neural network (many-to-one)\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, device, classes=None):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        self.device = device\n",
        "        self.classes = classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device) \n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device) \n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # shape = (batch_size, seq_length, hidden_size)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''Predict one label from one sample's features'''\n",
        "        # x: feature from a sample, LxN\n",
        "        #   L is length of sequency\n",
        "        #   N is feature dimension\n",
        "        x = torch.tensor(x[np.newaxis, :], dtype=torch.float32)\n",
        "        x = x.to(self.device)\n",
        "        outputs = self.forward(x)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predicted_index = predicted.item()\n",
        "        return predicted_index\n",
        "    \n",
        "    def set_classes(self, classes):\n",
        "        self.classes = classes \n",
        "    \n",
        "    def predict_audio_label(self, audio):\n",
        "        idx = self.predict_audio_label_index(audio)\n",
        "        assert self.classes, \"Classes names are not set. Don't know what audio label is\"\n",
        "        label = self.classes[idx]\n",
        "        return label\n",
        "\n",
        "    def predict_audio_label_index(self, audio):\n",
        "        audio.compute_mfcc()\n",
        "        x = audio.mfcc.T # (time_len, feature_dimension)\n",
        "        idx = self.predict(x)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzM_aOww-Zm8"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, eval_loader, num_to_eval=-1):\n",
        "    ''' Eval model on a dataset '''\n",
        "    device = model.device\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (featuress, labels) in enumerate(eval_loader):\n",
        "\n",
        "        featuress = featuress.to(device) # (batch, seq_len, input_size)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Predict\n",
        "        outputs = model(featuress)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # stop\n",
        "        if i+1 == num_to_eval:\n",
        "            break\n",
        "    eval_accu = correct / total\n",
        "    print('  Evaluate on eval or test dataset with {} samples: Accuracy = {}%'.format(\n",
        "        i+1, 100 * eval_accu)) \n",
        "    return eval_accu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od1-gIEo-Zm9"
      },
      "outputs": [],
      "source": [
        "def fix_weights_except_fc(model):\n",
        "    not_fix = \"fc\"\n",
        "    for name, param in model.state_dict().items():\n",
        "        if not_fix in name:\n",
        "            continue\n",
        "        else:\n",
        "            print(f\"Fix {name} layer\", end='. ')\n",
        "            param.requires_grad = False\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzNtA8KC-Zm9"
      },
      "outputs": [],
      "source": [
        "def train_model(model, args, train_loader, eval_loader):\n",
        "\n",
        "    device = model.device\n",
        "    logger = lib.TrainingLog(training_args=args)\n",
        "    if args.finetune_model:\n",
        "        fix_weights_except_fc(model)\n",
        "        \n",
        "    # -- create folder for saving model\n",
        "    if args.save_model_to:\n",
        "        if not os.path.exists(args.save_model_to):\n",
        "            os.makedirs(args.save_model_to)\n",
        "            \n",
        "    # -- Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # -- For updating learning rate\n",
        "    def update_lr(optimizer, lr):    \n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    # -- Train the model\n",
        "    total_step = len(train_loader)\n",
        "    curr_lr = args.learning_rate\n",
        "    cnt_batches = 0\n",
        "    for epoch in range(1, 1+args.num_epochs):\n",
        "        cnt_correct, cnt_total = 0, 0\n",
        "        for i, (featuress, labels) in enumerate(train_loader):\n",
        "            cnt_batches += 1\n",
        "\n",
        "            ''' original code of pytorch-tutorial:\n",
        "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "            labels = labels.to(device)\n",
        "            # we can see that the shape of images should be: \n",
        "            #    (batch_size, sequence_length, input_size)\n",
        "            '''\n",
        "            featuress = featuress.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(featuress)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward and optimize\n",
        "            loss.backward() # error\n",
        "            if cnt_batches % args.gradient_accumulations == 0:\n",
        "                # Accumulates gradient before each step\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Record result\n",
        "            _, argmax = torch.max(outputs, 1)\n",
        "            cnt_correct += (labels == argmax.squeeze()).sum().item()\n",
        "            cnt_total += labels.size(0)\n",
        "            \n",
        "            # Print accuracy\n",
        "            train_accu = cnt_correct/cnt_total\n",
        "            if (i+1) % 50 == 0 or (i+1) == len(train_loader):\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss = {:.4f}, Train accuracy = {:.2f}' \n",
        "                    .format(epoch, args.num_epochs, i+1, total_step, loss.item(), 100*train_accu))\n",
        "            continue\n",
        "        print(f\"Epoch {epoch} completes\")\n",
        "        \n",
        "        # -- Decay learning rate\n",
        "        if (epoch) % args.learning_rate_decay_interval == 0:\n",
        "            curr_lr *= args.learning_rate_decay_rate # lr = lr * rate\n",
        "            update_lr(optimizer, curr_lr)\n",
        "    \n",
        "        # -- Evaluate and save model\n",
        "        if (epoch) % 1 == 0 or (epoch) == args.num_epochs:\n",
        "            eval_accu = evaluate_model(model, eval_loader, num_to_eval=-1)\n",
        "            if args.save_model_to:\n",
        "                name_to_save = args.save_model_to + \"/\" + \"{:03d}\".format(epoch) + \".ckpt\"\n",
        "                torch.save(model.state_dict(), name_to_save)\n",
        "                print(\"Save model to: \", name_to_save)\n",
        "\n",
        "            # logger record\n",
        "            logger.store_accuracy(epoch, train=train_accu, eval=eval_accu)\n",
        "            logger.save_log(args.save_log_to)\n",
        "            \n",
        "            # logger Plot\n",
        "            if args.plot_accu and epoch == 1:\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                plt.ion()\n",
        "                if args.show_plotted_accu:\n",
        "                    plt.show()\n",
        "            if (epoch == args.num_epochs) or (args.plot_accu and epoch>1):\n",
        "                logger.plot_train_eval_accuracy()\n",
        "                if args.show_plotted_accu:\n",
        "                    plt.pause(0.01) \n",
        "                plt.savefig(fname=args.save_fig_to)\n",
        "        \n",
        "        # An epoch end\n",
        "        print(\"-\"*80 + \"\\n\")\n",
        "    \n",
        "    # Training end\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVh7pry0-Zm9"
      },
      "outputs": [],
      "source": [
        "# Set arguments ------------------------- \n",
        "args = set_default_args()\n",
        "args.learning_rate = 0.001\n",
        "args.num_epochs = 15\n",
        "args.learning_rate_decay_interval = 5 # decay for every 3 epochs\n",
        "args.learning_rate_decay_rate = 0.5 # lr = lr * rate\n",
        "args.do_data_augment = True\n",
        "args.train_eval_test_ratio=[0.9, 0.1, 0.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ufVHnq7-Zm9"
      },
      "outputs": [],
      "source": [
        "# current working directory\n",
        "DIR = os.path.abspath(os.getcwd())\n",
        "DATASET_DIRECTORY_PATH = DIR+'/data/speech_commands'\n",
        "args.data_folder = \"data/speech_commands/\"\n",
        "args.classes_txt = \"config/classes.names\"\n",
        "args.load_weights_from = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyreVpJx-Zm-",
        "outputId": "ce27631a-3ecb-40a3-cb76-37d567660c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load data from:  data/speech_commands/\n",
            "\tClasses:  on, no, yes, down, go, left, right, stop, up, off\n"
          ]
        }
      ],
      "source": [
        "# Dataset -------------------------- \n",
        "# Get data's filenames and labels\n",
        "files_name, files_label = lib.AudioDataset.load_filenames_and_labels(\n",
        "    args.data_folder, args.classes_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7X62orG-Zm-"
      },
      "outputs": [],
      "source": [
        "DEBUG = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra5fIUe9-Zm-"
      },
      "outputs": [],
      "source": [
        "# DEBUG: use only a subset of all data\n",
        "if DEBUG == True:\n",
        "    GAP = 1000\n",
        "    files_name = files_name[::GAP]\n",
        "    files_label = files_label[::GAP]\n",
        "    args.num_epochs = 5\n",
        "    print('We consider subset of dataset')\n",
        "else:\n",
        "    print('We consider the full dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jSVNYSP-Zm-"
      },
      "outputs": [],
      "source": [
        "# Set data augmentation\n",
        "if args.do_data_augment:\n",
        "    Aug = lib.Augmenter # rename\n",
        "    aug = Aug([        \n",
        "        Aug.Shift(rate=0.2, keep_size=False), \n",
        "        Aug.PadZeros(time=(0, 0.3)),\n",
        "        Aug.Amplify(rate=(0.2, 1.5)),\n",
        "        # Aug.PlaySpeed(rate=(0.7, 1.3), keep_size=False),\n",
        "        Aug.Noise(noise_folder=\"data/_background_noise_/\", \n",
        "                        prob_noise=0.7, intensity=(0, 0.7)),\n",
        "    ], prob_to_aug=0.8)\n",
        "else:\n",
        "    aug = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56-OqRx8-Zm-"
      },
      "outputs": [],
      "source": [
        "# Split data into train/eval/test\n",
        "tr_X, tr_Y, ev_X, ev_Y, te_X, te_Y = lib.split_train_eval_test(\n",
        "    X=files_name, Y=files_label, ratios=args.train_eval_test_ratio, dtype='list')\n",
        "train_dataset = lib.AudioDataset(files_name=tr_X, files_label=tr_Y, transform=aug)\n",
        "eval_dataset = lib.AudioDataset(files_name=ev_X, files_label=ev_Y, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFYXvdt4-Zm_"
      },
      "outputs": [],
      "source": [
        "len(tr_X),len(tr_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlnXmv4j-Zm_"
      },
      "outputs": [],
      "source": [
        "len(ev_X),len( ev_Y )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO8GQwVK-Zm_"
      },
      "outputs": [],
      "source": [
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "eval_loader = torch.utils.data.DataLoader(dataset=eval_dataset, batch_size=args.batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbbfa0FlDMPa"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58esZcUkDPJd"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1ZZF66wDT27"
      },
      "outputs": [],
      "source": [
        "cuda0 = torch.device('cuda:0')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2voIfat-Zm_"
      },
      "outputs": [],
      "source": [
        "# Create model and train -------------------------------------------------\n",
        "model = create_RNN_model(args, load_weights_from=args.load_weights_from) # create model\n",
        "train_model(model, args, train_loader, eval_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V8luKGH-Zm_"
      },
      "source": [
        "# 6. Test\n",
        "Let's test the model on an audio file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP5mmWFf-Zm_"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "\n",
        "def setup_classifier(load_weights_from):\n",
        "    model_args = set_default_args()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = create_RNN_model(model_args, load_weights_from)\n",
        "    return model\n",
        "\n",
        "def setup_classes_labels(load_classes_from, model):\n",
        "    classes = lib.read_list(load_classes_from)\n",
        "    print(f\"{len(classes)} classes: {classes}\")\n",
        "    model.set_classes(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1qzOfoA-ZnA"
      },
      "outputs": [],
      "source": [
        "model = setup_classifier(load_weights_from=\"checkpoints//015.ckpt\")\n",
        "setup_classes_labels(load_classes_from=\"config/classes.names\", model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5k5wF0Q-ZnA"
      },
      "outputs": [],
      "source": [
        "#Load test audio file\n",
        "if os.name == 'nt':\n",
        "    print(\"We are on Windows\")\n",
        "    test_file=  ev_X[0]\n",
        "    audio = lib.AudioClass(filename=test_file)\n",
        "else:\n",
        "    print(\"We are on Unix\")\n",
        "    test_file=ev_X[0]\n",
        "    audio = lib.AudioClass(filename=test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arlM8J9O-ZnA"
      },
      "outputs": [],
      "source": [
        "# Test on an audio\n",
        "ipd.Audio(audio.data, rate=audio.sample_rate) # play audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXx45dvY-ZnA"
      },
      "outputs": [],
      "source": [
        "# Test on an audio \n",
        "model.predict_audio_label(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmShNIwZ-ZnA"
      },
      "outputs": [],
      "source": [
        "label = model.predict_audio_label(audio)\n",
        "print(f\"Predicted label is: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkB8iZvj8zJp"
      },
      "source": [
        "# Predictions on the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plwYn6zX8zJp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JV9Puts8zJp"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "y_true = []\n",
        "for i in range(len(ev_X)):\n",
        "    audio = lib.AudioClass(filename=ev_X[i])\n",
        "    prediction=model.predict_audio_label(audio)\n",
        "    y_pred.append(prediction)\n",
        "    real=labels[ev_Y[i]]\n",
        "    y_true.append(real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEa41fRj8zJq"
      },
      "outputs": [],
      "source": [
        "# converting list to array\n",
        "actual  = numpy.array(y_true)\n",
        "predicted = numpy.array(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtfhY--48zJq"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = labels)\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(actual, predicted))"
      ],
      "metadata": {
        "id": "H48WW6c0cre3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python (Pytorch)",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}